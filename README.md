# VSCode Ollama Extension

<p align="center">
  <img src="resources/logo.png" alt="VSCode Ollama Logo" width="128"/>
</p>

<p align="center">
  <a href="https://marketplace.visualstudio.com/items?itemName=warm3snow.vscode-ollama">
    <img src="https://img.shields.io/visual-studio-marketplace/i/warm3snow.vscode-ollama?logo=visual-studio-code" alt="Downloads"/>
  </a>
  <a href="https://marketplace.visualstudio.com/items?itemName=warm3snow.vscode-ollama">
    <img src="https://img.shields.io/visual-studio-marketplace/r/warm3snow.vscode-ollama?logo=visual-studio-code" alt="Rating"/>
  </a>
  <a href="https://github.com/warm3snow/vscode-ollama">
    <img src="https://img.shields.io/github/stars/warm3snow/vscode-ollama?style=social" alt="GitHub stars"/>
  </a>
  <a href="https://github.com/warm3snow/vscode-ollama/blob/main/LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License: MIT"/>
  </a>
</p>

[English](README.md) | [ä¸­æ–‡](README_CN.md)

VSCode Ollama is a powerful Visual Studio Code extension that seamlessly integrates Ollama's local LLM capabilities into your development environment.

## âœ¨ Features

- ğŸ¤– **Local LLM Support**
  - Local model execution based on Ollama
  - Multiple model switching support
  - Low-latency responses

- ğŸ” **Web Search** [Coming Soon]
  - Real-time web information integration
  - Smart search results synthesis
  - Accurate information citation

- ğŸ’¡ **Intelligent Chat**
  - Streaming response output
  - Thought process visualization
  - Chat history preservation

- âš™ï¸ **Flexible Configuration**
  - Custom server address
  - Adjustable performance modes
  - Model parameter configuration

## ğŸš€ Quick Start

1. **Install Ollama**
   ```bash
   # macOS
   brew install ollama

   # Linux
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **Install Extension**
   - Open Extensions in VS Code
   - Search for "VSCode Ollama"
   - Click Install

3. **Configure Extension**
   - Open Command Palette (Ctrl+Shift+P / Cmd+Shift+P)
   - Type "Ollama: Settings"
   - Configure server address and default model

4. **Start Using**
   - Use command "Ollama: Open Chat" to start conversation
   - Select model in chat interface
   - Toggle web search
   - Send message to interact

## ğŸ“ Usage

### Commands
- `Ollama: Open Chat` - Open chat interface
- `Ollama: Settings` - Open settings page
- `Ollama: Test Connection` - Test server connection

### Shortcuts
- `Shift + Enter` - New line in chat input
- `Enter` - Send message

## â¤ï¸ Support & Donation

If you find this extension helpful, you can support the developer by:

<details>
<summary>ğŸ’° Donation Methods</summary>

<p align="center">Support the developer</p>

<div style="display: flex; justify-content: space-around; margin: 20px 0;">
  <div style="text-align: center; margin: 0 40px;">
    <img src="resources/wechat.jpg" alt="WeChat Pay" width="240"/>
    <br/>
    <br/>
    WeChat Pay
  </div>
  <div style="text-align: center; margin: 0 40px;">
    <img src="resources/alipay.jpg" alt="Alipay" width="240"/>
    <br/>
    <br/>
    Alipay
  </div>
</div>

### ğŸª™ Cryptocurrency

<details>
<summary>Bitcoin</summary>

- **Native Segwit**  
  `bc1qskds324wteq5kfmxh63g624htzwd34gky0f0q5`
  
- **Taproot**  
  `bc1pk0zud9csztjrkqew54v2nv7g3kq0xc2n80jatkmz9axkve4trfcqp0aksf`
</details>

<details>
<summary>Ethereum</summary>

`0xB0DA3bbC5e9f8C4b4A12d493A72c33dBDf1A9803`
</details>

<details>
<summary>Solana</summary>

`AMvPLymJm4TZZgvrYU7DCVn4uuzh6gfJiHWNK35gmUzd`
</details>

</details>

- â­ Star the [GitHub repository](https://github.com/warm3snow/vscode-ollama)
- ğŸ“ Submit issues or feedback
- ğŸš€ Contribute to the codebase
- ğŸ’¬ Share with your friends

Your support helps maintain and improve this extension! Thank you! â¤ï¸