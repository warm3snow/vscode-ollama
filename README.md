# VSCode Ollama Extension

<p align="center">
  <img src="resources/logo.png" alt="VSCode Ollama Logo" width="128"/>
</p>

<p align="center">
  <a href="https://marketplace.visualstudio.com/items?itemName=warm3snow.vscode-ollama">
    <img src="https://img.shields.io/visual-studio-marketplace/i/warm3snow.vscode-ollama?logo=visual-studio-code" alt="Downloads"/>
  </a>
  <a href="https://marketplace.visualstudio.com/items?itemName=warm3snow.vscode-ollama">
    <img src="https://img.shields.io/visual-studio-marketplace/r/warm3snow.vscode-ollama?logo=visual-studio-code" alt="Rating"/>
  </a>
  <a href="https://github.com/warm3snow/vscode-ollama">
    <img src="https://img.shields.io/github/stars/warm3snow/vscode-ollama?style=social" alt="GitHub stars"/>
  </a>
  <a href="https://github.com/warm3snow/vscode-ollama/blob/main/LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License: MIT"/>
  </a>
</p>

[English](README.md) | [ä¸­æ–‡](README_CN.md)

VSCode Ollama is a powerful Visual Studio Code extension that seamlessly integrates Ollama's local LLM capabilities into your development environment.

## âœ¨ Features

- ğŸ¤– **Local LLM Support**
  - Local model execution based on Ollama
  - Multiple model switching support
  - Low-latency responses

- ğŸ” **Web Search** 
  - Real-time web information integration
  - Smart search results synthesis
  - Accurate information citation

- ğŸ’¡ **Intelligent Chat**
  - Streaming response output
  - Thought process visualization
  - Chat history preservation

- âš™ï¸ **Flexible Configuration**
  - Custom server address
  - Adjustable performance modes
  - Model parameter configuration

## ğŸš€ Quick Start

## ğŸ“º Tutorial

1. **Install Ollama**
   ```bash
   # macOS
   brew install ollama

   # Linux
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **Install Extension**
   - Open Extensions in VS Code
   - Search for "VSCode Ollama"
   - Click Install

3. **Configure Extension**
   - Open Command Palette (Ctrl+Shift+P / Cmd+Shift+P)
   - Type "Ollama: Settings"
   - Configure server address and default model

4. **Start Using**
   - Use command "Ollama: Open Chat" to start conversation
   - Select model in chat interface
   - Toggle web search
   - Send message to interact

   <p align="center">
     <img src="resources/chat.png" alt="VSCode Ollama Chat Interface" width="800"/>
   </p>

## ğŸ“ Usage

### Commands
- `Ollama: Open Chat` - Open chat interface
- `Ollama: Settings` - Open settings page
- `Ollama: Test Connection` - Test server connection

### Shortcuts
- `Shift + Enter` - New line in chat input
- `Enter` - Send message

## â¤ï¸ Support & Donation

If you find this extension helpful, you can support the developer by:

<details>
<summary>ğŸ’° Donation Methods</summary>

<p align="center">Support the developer</p>

<table>
  <tr>
    <td align="center">
      <img src="resources/wechat.jpg" alt="WeChat Pay" width="240"/>
      <br/>
      WeChat Pay
    </td>
    <td align="center">
      <img src="resources/alipay.jpg" alt="Alipay" width="240"/>
      <br/>
      Alipay
    </td>
  </tr>
</table>

### ğŸª™ Cryptocurrency

<table>
  <tr>
    <td>
      <b>Bitcoin</b>
    </td>
    <td>
      <b>Native Segwit</b><br/>
      <code>bc1qskds324wteq5kfmxh63g624htzwd34gky0f0q5</code>
      <br/><br/>
      <b>Taproot</b><br/>
      <code>bc1pk0zud9csztjrkqew54v2nv7g3kq0xc2n80jatkmz9axkve4trfcqp0aksf</code>
    </td>
  </tr>
  <tr>
    <td>
      <b>Ethereum</b>
    </td>
    <td>
      <code>0xB0DA3bbC5e9f8C4b4A12d493A72c33dBDf1A9803</code>
    </td>
  </tr>
  <tr>
    <td>
      <b>Solana</b>
    </td>
    <td>
      <code>AMvPLymJm4TZZgvrYU7DCVn4uuzh6gfJiHWNK35gmUzd</code>
    </td>
  </tr>
</table>

</details>

Your support helps maintain and improve this extension! Thank you! â¤ï¸
- â­ Star the [GitHub repository](https://github.com/warm3snow/vscode-ollama)
- ğŸ“ Submit issues or feedback
- ğŸš€ Contribute to the codebase
- ğŸ’¬ Share with your friends

## ğŸ“ Release Notes

See [CHANGELOG.md](CHANGELOG.md) for release notes.

## ğŸ“ License

This extension is licensed under the [MIT License](LICENSE).