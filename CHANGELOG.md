# Change Log

All notable changes to the "vscode-ollama" extension will be documented in this file.

Check [Keep a Changelog](http://keepachangelog.com/) for recommendations on how to structure this file.

## [1.0.0] - 2025-02-17

### Added
- Initial release with core functionalities
- Local LLM support with Ollama integration
  - Multiple model support and easy switching
  - Low-latency responses
  - Secure and private environment
- Web search capability(Coming Soon!!!)
  - Real-time information integration
  - Smart search results synthesis
  - Accurate information citation
- Streaming chat interface
  - Markdown support
  - Code-aware context
  - Syntax highlighting
- Chat history preservation
  - Local storage
  - Session management
  - Export functionality
- Thought process visualization
  - Step-by-step reasoning
  - Intermediate results display
- Customizable settings
  - Server configuration
  - Model selection
  - Performance modes
  - Token management
  - Keep-alive settings

### Technical
- VS Code API integration
- WebView implementation
- Ollama API integration
- Error handling and recovery
- Performance optimizations